{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP ONE : IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import InputLayer, Embedding, LSTM, RepeatVector, TimeDistributed, Dense, Attention, Concatenate, Input, Bidirectional\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2 : FILE READING AND DATA COLLECTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  English words/sentences  \\\n",
      "80881                        Are you ready for Christmas?   \n",
      "21864                                  Where is the book?   \n",
      "51144                             We became good friends.   \n",
      "51667                             Where did everybody go?   \n",
      "163369  If I could, I would let every caged bird fly f...   \n",
      "\n",
      "                                   French words/sentences  \n",
      "80881                         Êtes-vous prête pour Noël ?  \n",
      "21864                                  Où est le livret ?  \n",
      "51144                            Nous devînmes bons amis.  \n",
      "51667                     Où tout le monde est-il parti ?  \n",
      "163369  Si je pouvais, je laisserais s'envoler libreme...  \n"
     ]
    }
   ],
   "source": [
    "english_french = pd.read_csv('C:/Users/user/Desktop/AI and Data Science Workshop/MyNLPModel/data/eng_-french.csv')\n",
    "english_french = english_french.sample(4000)\n",
    "print(english_french.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 : MAKING THE WORDS LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_french['French words/sentences'] = english_french['French words/sentences'].str.lower()\n",
    "english_french['English words/sentences'] = english_french['English words/sentences'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_french.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4 : TOKENIZING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = Tokenizer()\n",
    "french_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.fit_on_texts(english_french['English words/sentences'])\n",
    "french_tokenizer.fit_on_texts(english_french['French words/sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = english_tokenizer.texts_to_sequences(english_french['English words/sentences'])\n",
    "y = french_tokenizer.texts_to_sequences(english_french['French words/sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2967\n",
      "4513\n"
     ]
    }
   ],
   "source": [
    "print(len(english_tokenizer.word_index))\n",
    "print(len(french_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 : CONVERTING SENTENCES TO SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80881</th>\n",
       "      <td>are you ready for christmas?</td>\n",
       "      <td>êtes-vous prête pour noël ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21864</th>\n",
       "      <td>where is the book?</td>\n",
       "      <td>où est le livret ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51144</th>\n",
       "      <td>we became good friends.</td>\n",
       "      <td>nous devînmes bons amis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51667</th>\n",
       "      <td>where did everybody go?</td>\n",
       "      <td>où tout le monde est-il parti ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163369</th>\n",
       "      <td>if i could, i would let every caged bird fly f...</td>\n",
       "      <td>si je pouvais, je laisserais s'envoler libreme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  English words/sentences  \\\n",
       "80881                        are you ready for christmas?   \n",
       "21864                                  where is the book?   \n",
       "51144                             we became good friends.   \n",
       "51667                             where did everybody go?   \n",
       "163369  if i could, i would let every caged bird fly f...   \n",
       "\n",
       "                                   French words/sentences  \n",
       "80881                         êtes-vous prête pour noël ?  \n",
       "21864                                  où est le livret ?  \n",
       "51144                            nous devînmes bons amis.  \n",
       "51667                     où tout le monde est-il parti ?  \n",
       "163369  si je pouvais, je laisserais s'envoler libreme...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = english_tokenizer.texts_to_sequences(english_french['English words/sentences'])\n",
    "y = french_tokenizer.texts_to_sequences(english_french['French words/sentences'])\n",
    "english_french.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sequences sample: [[26, 2, 145, 20, 640], [107, 6, 4, 169], [24, 1004, 77, 258], [107, 38, 297, 45], [59, 1, 88, 1, 74, 104, 190, 1441, 791, 792, 393]]\n",
      "French sequences sample: [[41, 4, 424, 23, 705], [90, 13, 8, 1755], [17, 1756, 859, 333], [90, 35, 8, 119, 13, 10, 580], [42, 1, 425, 1, 1757, 1758, 1759, 73, 18, 1111, 19, 1112]]\n"
     ]
    }
   ],
   "source": [
    "print(\"English sequences sample:\", X[:5])\n",
    "print(\"French sequences sample:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_len = max(len(seq) for seq in X)\n",
    "max_fr_len = max(len(seq) for seq in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6 : PAD SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of English sequences: 22\n",
      "Max length of French sequences: 26\n",
      "Shape of X_padded: (4000, 22)\n",
      "Shape of y_padded: (4000, 26)\n"
     ]
    }
   ],
   "source": [
    "X_padded = pad_sequences(X, maxlen=max_eng_len, padding='post')\n",
    "y_padded = pad_sequences(y, maxlen=max_fr_len, padding='post')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Max length of English sequences:\", max_eng_len)\n",
    "print(\"Max length of French sequences:\", max_fr_len)\n",
    "print(\"Shape of X_padded:\", X_padded.shape)\n",
    "print(\"Shape of y_padded:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English padded shape: (500, 16)\n",
      "French padded shape: (500, 22)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes after padding\n",
    "print(\"English padded shape:\", X_padded.shape)\n",
    "print(\"French padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 10 : TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_eng_len,)))\n",
    "model.add(Embedding(input_dim=len(english_tokenizer.word_index) + 1, output_dim=128))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(max_fr_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(french_tokenizer.word_index) + 1, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 12 : DEFINING THE PRE-PROCESS_INPUT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    token_ids = [english_tokenizer.word_index.get(word, 0) for word in tokens]\n",
    "    padded_token_ids = pad_sequences([token_ids], maxlen=max_eng_len, padding='post')\n",
    "    return padded_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 13 : DEFINING THE DECODE_SEQUENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decode_sequence function\n",
    "def decode_sequence(input_seq):\n",
    "    print(f\"Input sequence to decode: {input_seq}\")  # Debug statement\n",
    "    decoded_sentence = []\n",
    "    for idx in input_seq:\n",
    "        if idx > 0 and idx in french_index_to_word:\n",
    "            word = french_index_to_word[idx]\n",
    "            decoded_sentence.append(word)\n",
    "    translated_sentence = ' '.join(decoded_sentence)\n",
    "    print(f\"Decoded sentence: {translated_sentence}\")  # Debug statement\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14 : TRANSLATE USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len):\n",
    "    # Tokenize and pad the input sequence\n",
    "    input_seq = english_tokenizer.texts_to_sequences([user_input])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "    # Predict the translation\n",
    "    prediction = model.predict(input_seq)\n",
    "    \n",
    "    # Decode the prediction to get the translated sentence\n",
    "    translated_seq = np.argmax(prediction, axis=-1)\n",
    "    translated_sentence = \"\"\n",
    "    \n",
    "    for word_id in translated_seq[0]:\n",
    "        if word_id == 0:\n",
    "            break\n",
    "        translated_sentence += french_tokenizer.index_word[word_id] + ' '\n",
    "    \n",
    "    return translated_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 11 : CHECKING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">379,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4514</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">582,306</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m379,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_4 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m4514\u001b[0m)       │       \u001b[38;5;34m582,306\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,225,378</span> (4.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,225,378\u001b[0m (4.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,225,378</span> (4.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,225,378\u001b[0m (4.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 15 : RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter an English sentence to translate: \")\n",
    "translated_sentence = translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len)\n",
    "print(f\"Translated to French: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
