{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP ONE : IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2 : FILE READING AND DATA COLLECTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               English words/sentences  \\\n",
      "42567                           I've run out of money.   \n",
      "158190  Tell Tom that he needs to wear a tie tomorrow.   \n",
      "156947   Who's that cute guy I saw you with yesterday?   \n",
      "85670                     There is no reason to shout.   \n",
      "38529                            What a big boy he is!   \n",
      "\n",
      "                                   French words/sentences  \n",
      "42567                     Je suis tombé à court d'argent.  \n",
      "158190  Dites à Tom qu'il lui faut porter une cravate ...  \n",
      "156947  Qui est ce mignon garçon avec lequel je t'ai v...  \n",
      "85670                       Il n'y a pas besoin de crier.  \n",
      "38529                          Quel grand garçon il est !  \n"
     ]
    }
   ],
   "source": [
    "english_french = pd.read_csv('C:/Users/user/Desktop/AI and Data Science Workshop/MyNLPModel/data/eng_-french.csv')\n",
    "english_french = english_french.sample(4000)\n",
    "print(english_french.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 : MAKING THE WORDS LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_french['French words/sentences'] = english_french['French words/sentences'].str.lower()\n",
    "english_french['English words/sentences'] = english_french['English words/sentences'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_french.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4 : TOKENIZING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = Tokenizer()\n",
    "french_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.fit_on_texts(english_french['English words/sentences'])\n",
    "french_tokenizer.fit_on_texts(english_french['French words/sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 : CONVERTING SENTENCES TO SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42567</th>\n",
       "      <td>i've run out of money.</td>\n",
       "      <td>je suis tombé à court d'argent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158190</th>\n",
       "      <td>tell tom that he needs to wear a tie tomorrow.</td>\n",
       "      <td>dites à tom qu'il lui faut porter une cravate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156947</th>\n",
       "      <td>who's that cute guy i saw you with yesterday?</td>\n",
       "      <td>qui est ce mignon garçon avec lequel je t'ai v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85670</th>\n",
       "      <td>there is no reason to shout.</td>\n",
       "      <td>il n'y a pas besoin de crier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38529</th>\n",
       "      <td>what a big boy he is!</td>\n",
       "      <td>quel grand garçon il est !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               English words/sentences  \\\n",
       "42567                           i've run out of money.   \n",
       "158190  tell tom that he needs to wear a tie tomorrow.   \n",
       "156947   who's that cute guy i saw you with yesterday?   \n",
       "85670                     there is no reason to shout.   \n",
       "38529                            what a big boy he is!   \n",
       "\n",
       "                                   French words/sentences  \n",
       "42567                     je suis tombé à court d'argent.  \n",
       "158190  dites à tom qu'il lui faut porter une cravate ...  \n",
       "156947  qui est ce mignon garçon avec lequel je t'ai v...  \n",
       "85670                       il n'y a pas besoin de crier.  \n",
       "38529                          quel grand garçon il est !  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = english_tokenizer.texts_to_sequences(english_french['English words/sentences'])\n",
    "y = french_tokenizer.texts_to_sequences(english_french['French words/sentences'])\n",
    "english_french.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sequences sample: [[83, 386, 68, 12, 118], [75, 8, 7, 14, 329, 3, 630, 5, 469, 193], [533, 7, 631, 756, 1, 280, 2, 32, 186], [60, 6, 53, 632, 3, 987], [19, 5, 281, 305, 14, 6]]\n",
      "French sequences sample: [[1, 21, 668, 6, 490, 190], [429, 6, 14, 42, 65, 131, 560, 20, 837, 191], [33, 13, 12, 838, 315, 41, 669, 1, 179, 561, 164], [11, 126, 16, 3, 105, 2, 1718], [132, 180, 315, 11, 1719]]\n"
     ]
    }
   ],
   "source": [
    "print(\"English sequences sample:\", X[:5])\n",
    "print(\"French sequences sample:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_len = max(len(seq) for seq in X)\n",
    "max_fr_len = max(len(seq) for seq in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6 : PAD SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of English sequences: 22\n",
      "Max length of French sequences: 22\n",
      "Shape of X_padded: (4000, 22)\n",
      "Shape of y_padded: (4000, 22)\n"
     ]
    }
   ],
   "source": [
    "X_padded = pad_sequences(X, maxlen=max_eng_len, padding='post')\n",
    "y_padded = pad_sequences(y, maxlen=max_fr_len, padding='post')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Max length of English sequences:\", max_eng_len)\n",
    "print(\"Max length of French sequences:\", max_fr_len)\n",
    "print(\"Shape of X_padded:\", X_padded.shape)\n",
    "print(\"Shape of y_padded:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English padded shape: (500, 16)\n",
      "French padded shape: (500, 22)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes after padding\n",
    "print(\"English padded shape:\", X_padded.shape)\n",
    "print(\"French padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 10 : TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_eng_len,)))\n",
    "model.add(Embedding(input_dim=len(english_tokenizer.word_index) + 1, output_dim=128))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(max_fr_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(len(french_tokenizer.word_index) + 1, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">392,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4553</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">587,337</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m392,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_7 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m4553\u001b[0m)       │       \u001b[38;5;34m587,337\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,242,569</span> (4.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,242,569\u001b[0m (4.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,242,569</span> (4.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,242,569\u001b[0m (4.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 12 : DEFINING THE PRE-PROCESS_INPUT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentence, english_tokenizer, max_eng_len):\n",
    "    tokens = sentence.lower().split()\n",
    "    token_ids = [english_tokenizer.word_index.get(word, 0) for word in tokens]\n",
    "    padded_token_ids = pad_sequences([token_ids], maxlen=max_eng_len, padding='post')\n",
    "    return padded_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 13 : DEFINING THE DECODE_SEQUENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model, french_tokenizer, max_fr_len):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = model.layers[2].predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = french_tokenizer.word_index['<start>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = model.layers[4].predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = french_tokenizer.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_char == '<end>' or len(decoded_sentence.split()) > max_fr_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14 : TRANSLATE USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_user_input(input_sentence, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len):\n",
    "    encoded_input = preprocess_input(input_sentence, english_tokenizer, max_eng_len)\n",
    "    translated_sentence = decode_sequence(encoded_input, model, french_tokenizer, max_fr_len)\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 11 : CHECKING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">392,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4553</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">587,337</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m392,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_7 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m4553\u001b[0m)       │       \u001b[38;5;34m587,337\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,242,569</span> (4.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,242,569\u001b[0m (4.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,242,569</span> (4.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,242,569\u001b[0m (4.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 15 : RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: hello\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RepeatVector' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter an English sentence to translate: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated to French: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[94], line 3\u001b[0m, in \u001b[0;36mtranslate_user_input\u001b[1;34m(input_sentence, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_user_input\u001b[39m(input_sentence, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len):\n\u001b[0;32m      2\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m preprocess_input(input_sentence, english_tokenizer, max_eng_len)\n\u001b[1;32m----> 3\u001b[0m     translated_sentence \u001b[38;5;241m=\u001b[39m decode_sequence(encoded_input, model, french_tokenizer, max_fr_len)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m translated_sentence\n",
      "Cell \u001b[1;32mIn[93], line 3\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq, model, french_tokenizer, max_fr_len)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_sequence\u001b[39m(input_seq, model, french_tokenizer, max_fr_len):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Encode the input as state vectors.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     states_value \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(input_seq)\n\u001b[0;32m      4\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m     target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m french_tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RepeatVector' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter an English sentence to translate: \")\n",
    "print(f\"User Input: {user_input}\")\n",
    "translated_sentence = translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len)\n",
    "print(f\"Translated to French: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
