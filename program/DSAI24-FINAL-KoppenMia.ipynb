{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP ONE : IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, RepeatVector, InputLayer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2 : FILE READING AND DATA COLLECTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87737</th>\n",
       "      <td>A little work won't kill you.</td>\n",
       "      <td>Un peu de travail ne te tuera pas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122156</th>\n",
       "      <td>They make good use of their rooms.</td>\n",
       "      <td>Ils utilisent leurs chambres à bon escient.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27886</th>\n",
       "      <td>Bring me some water.</td>\n",
       "      <td>Apporte-moi de l'eau !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65244</th>\n",
       "      <td>What more could you want?</td>\n",
       "      <td>Que pourrais-tu vouloir de plus ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81668</th>\n",
       "      <td>He didn't have a single pen.</td>\n",
       "      <td>Il ne disposait d'aucun stylo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   English words/sentences  \\\n",
       "87737        A little work won't kill you.   \n",
       "122156  They make good use of their rooms.   \n",
       "27886                 Bring me some water.   \n",
       "65244            What more could you want?   \n",
       "81668         He didn't have a single pen.   \n",
       "\n",
       "                             French words/sentences  \n",
       "87737            Un peu de travail ne te tuera pas.  \n",
       "122156  Ils utilisent leurs chambres à bon escient.  \n",
       "27886                        Apporte-moi de l'eau !  \n",
       "65244             Que pourrais-tu vouloir de plus ?  \n",
       "81668                Il ne disposait d'aucun stylo.  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_french = pd.read_csv('C:/Users/user/Desktop/AI and Data Science Workshop/MyNLPModel/data/eng_-french.csv')\n",
    "english_french = english_french.sample(4000)\n",
    "english_french.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 : MAKING THE WORDS LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_french['French words/sentences'] = english_french['French words/sentences'].str.lower()\n",
    "english_french['English words/sentences'] = english_french['English words/sentences'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_french.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4 : TOKENIZING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = Tokenizer()\n",
    "french_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer.fit_on_texts(english_french['English words/sentences'])\n",
    "french_tokenizer.fit_on_texts(english_french['French words/sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5 : CONVERTING SENTENCES TO SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80498</th>\n",
       "      <td>you must study much harder.</td>\n",
       "      <td>tu dois étudier beaucoup plus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>how's the family?</td>\n",
       "      <td>comment va la famille ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86617</th>\n",
       "      <td>we leave tomorrow afternoon.</td>\n",
       "      <td>nous partons demain après-midi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17076</th>\n",
       "      <td>what if i refuse?</td>\n",
       "      <td>et si je refuse ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38661</th>\n",
       "      <td>what you say is true.</td>\n",
       "      <td>ce que tu dis est vrai.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            English words/sentences           French words/sentences\n",
       "80498   you must study much harder.   tu dois étudier beaucoup plus.\n",
       "14313             how's the family?          comment va la famille ?\n",
       "86617  we leave tomorrow afternoon.  nous partons demain après-midi.\n",
       "17076             what if i refuse?                et si je refuse ?\n",
       "38661         what you say is true.          ce que tu dis est vrai."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = english_tokenizer.texts_to_sequences(english_french['English words/sentences'])\n",
    "y = french_tokenizer.texts_to_sequences(english_french['French words/sentences'])\n",
    "english_french.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sequences sample: [[2, 117, 535, 98, 1434], [536, 4, 304], [27, 172, 256, 537], [22, 70, 1, 981], [22, 2, 137, 6, 223]]\n",
      "French sequences sample: [[9, 155, 1106, 100, 34], [67, 108, 11, 1718], [17, 1719, 333, 186, 554], [56, 50, 1, 1720], [13, 5, 9, 257, 12, 216]]\n"
     ]
    }
   ],
   "source": [
    "print(\"English sequences sample:\", X[:5])\n",
    "print(\"French sequences sample:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_len = max(len(seq) for seq in X)\n",
    "max_fr_len = max(len(seq) for seq in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6 : PAD SEQUENCES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of English sequences: 32\n",
      "Max length of French sequences: 31\n",
      "Shape of X_padded: (4000, 32)\n",
      "Shape of y_padded: (4000, 31)\n"
     ]
    }
   ],
   "source": [
    "X_padded = pad_sequences(X, maxlen=max_eng_len, padding='post')\n",
    "y_padded = pad_sequences(y, maxlen=max_fr_len, padding='post')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Max length of English sequences:\", max_eng_len)\n",
    "print(\"Max length of French sequences:\", max_fr_len)\n",
    "print(\"Shape of X_padded:\", X_padded.shape)\n",
    "print(\"Shape of y_padded:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 12 : DEFINING THE PRE-PROCESS_INPUT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentence, english_tokenizer, max_eng_len):\n",
    "    tokens = sentence.lower().split()\n",
    "    token_ids = [english_tokenizer.word_index.get(word, 0) for word in tokens]\n",
    "    padded_token_ids = pad_sequences([token_ids], maxlen=max_eng_len, padding='post')\n",
    "    return padded_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English padded shape: (4000, 32)\n",
      "French padded shape: (4000, 31)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes after padding\n",
    "print(\"English padded shape:\", X_padded.shape)\n",
    "print(\"French padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 10 : TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 525ms/step - accuracy: 0.7168 - loss: 5.8882 - val_accuracy: 0.7827 - val_loss: 1.9161\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 398ms/step - accuracy: 0.7887 - loss: 1.8025 - val_accuracy: 0.7827 - val_loss: 1.7110\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 395ms/step - accuracy: 0.7851 - loss: 1.6343 - val_accuracy: 0.7827 - val_loss: 1.6179\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 376ms/step - accuracy: 0.7860 - loss: 1.5612 - val_accuracy: 0.7880 - val_loss: 1.5959\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 394ms/step - accuracy: 0.7930 - loss: 1.5292 - val_accuracy: 0.7883 - val_loss: 1.5851\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 386ms/step - accuracy: 0.7923 - loss: 1.5217 - val_accuracy: 0.7883 - val_loss: 1.5852\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 376ms/step - accuracy: 0.7945 - loss: 1.4860 - val_accuracy: 0.7883 - val_loss: 1.5699\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 384ms/step - accuracy: 0.7938 - loss: 1.4794 - val_accuracy: 0.7883 - val_loss: 1.5748\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 375ms/step - accuracy: 0.7920 - loss: 1.4922 - val_accuracy: 0.7883 - val_loss: 1.5621\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 390ms/step - accuracy: 0.7943 - loss: 1.4576 - val_accuracy: 0.7883 - val_loss: 1.5630\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 386ms/step - accuracy: 0.7917 - loss: 1.4760 - val_accuracy: 0.7908 - val_loss: 1.5619\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 382ms/step - accuracy: 0.7909 - loss: 1.4914 - val_accuracy: 0.7898 - val_loss: 1.5645\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 388ms/step - accuracy: 0.7941 - loss: 1.4612 - val_accuracy: 0.7919 - val_loss: 1.5654\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 390ms/step - accuracy: 0.7946 - loss: 1.4624 - val_accuracy: 0.7919 - val_loss: 1.5647\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 374ms/step - accuracy: 0.7969 - loss: 1.4351 - val_accuracy: 0.7898 - val_loss: 1.5666\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 389ms/step - accuracy: 0.7991 - loss: 1.4234 - val_accuracy: 0.7919 - val_loss: 1.5681\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 386ms/step - accuracy: 0.7952 - loss: 1.4530 - val_accuracy: 0.7919 - val_loss: 1.5687\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 372ms/step - accuracy: 0.7955 - loss: 1.4436 - val_accuracy: 0.7919 - val_loss: 1.5736\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 398ms/step - accuracy: 0.7964 - loss: 1.4377 - val_accuracy: 0.7919 - val_loss: 1.5712\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 371ms/step - accuracy: 0.7979 - loss: 1.4214 - val_accuracy: 0.7919 - val_loss: 1.5739\n"
     ]
    }
   ],
   "source": [
    "y_padded_categorical = to_categorical(y_padded, num_classes=len(french_tokenizer.word_index) + 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(max_eng_len,)))\n",
    "model.add(Embedding(input_dim=len(english_tokenizer.word_index) + 1, output_dim=128))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(max_fr_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(french_tokenizer.word_index) + 1, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_padded, y_padded_categorical, batch_size=64, epochs=20, validation_split=0.2)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 13 : DEFINING THE DECODE_SEQUENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(prediction, tokenizer):\n",
    "    translated_sentence = []\n",
    "    \n",
    "    for word_prob in prediction:\n",
    "        word_id = np.argmax(word_prob)\n",
    "        if word_id == 0:\n",
    "            break\n",
    "        translated_sentence.append(tokenizer.index_word[word_id])\n",
    "    \n",
    "    return ' '.join(translated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14 : TRANSLATE USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len):\n",
    "    # Tokenize and pad the input sequence\n",
    "    input_seq = english_tokenizer.texts_to_sequences([user_input])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "    # Predict the translation\n",
    "    prediction = model.predict(input_seq)\n",
    "\n",
    "    # Decode the prediction to get the translated sentence\n",
    "    translated_sentence = decode_sequence(prediction[0], french_tokenizer)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 11 : CHECKING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">376,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4428</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">571,212</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m376,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector_2 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m4428\u001b[0m)       │       \u001b[38;5;34m571,212\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,210,956</span> (4.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,210,956\u001b[0m (4.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,210,956</span> (4.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,210,956\u001b[0m (4.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 15 : RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Translated to French: je ne pas\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter an English sentence to translate: \")\n",
    "translated_sentence = translate_user_input(user_input, model, english_tokenizer, french_tokenizer, max_eng_len, max_fr_len)\n",
    "print(f\"Translated to French: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
